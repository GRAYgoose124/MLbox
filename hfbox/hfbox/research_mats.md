https://github.com/thunlp/OpenPrompt
https://github.com/Significant-Gravitas/AutoGPT


https://arxiv.org/abs/2005.14165
https://arxiv.org/abs/2303.12712
https://arxiv.org/abs/2205.11916
https://arxiv.org/abs/2305.14992
https://arxiv.org/abs/2212.09196
https://arxiv.org/abs/2306.06548
https://arxiv.org/abs/2308.00304
https://arxiv.org/abs/2306.07536

https://arxiv.org/abs/2305.18654
- this sorta thing would explain to me the problem now. I mentioned how it was linearized and such...We just need better pipelining and executive mechanisms...Which are being worked on constantly.


https://arxiv.org/abs/2211.01910
https://arxiv.org/abs/2305.11159

https://arxiv.org/abs/2305.01555

https://arxiv.org/abs/2303.18223
https://arxiv.org/abs/2305.18486
https://arxiv.org/abs/2304.11116


https://www.sciencedirect.com/science/article/pii/S2666651021000140
https://arxiv.org/abs/2304.09842

https://arxiv.org/abs/2304.03262
https://arxiv.org/abs/2305.10276

https://arxiv.org/abs/2308.10792

https://arxiv.org/abs/2308.10835

https://arxiv.org/abs/2210.01240
https://arxiv.org/abs/2304.10464
https://arxiv.org/abs/2304.11116

https://arxiv.org/abs/2308.10088
https://arxiv.org/abs/2305.03495

https://arxiv.org/abs/2307.08072

https://arxiv.org/abs/2304.11107
- just interesting.

https://arxiv.org/abs/2307.10250
https://arxiv.org/abs/2306.00024

https://arxiv.org/abs/2306.16564

https://arxiv.org/abs/2304.11490
https://arxiv.org/abs/2304.09797

https://arxiv.org/abs/2308.07758
https://arxiv.org/abs/2203.11171
https://arxiv.org/abs/2210.06710
# #
https://arxiv.org/abs/2303.16416
- Key take away: BERTs are generally specialized, but ChatGPT performs well zero-shot on all tasks.
  - https://arxiv.org/abs/2307.12114
- https://arxiv.org/abs/2305.04928
- https://arxiv.org/abs/2305.03253
- https://arxiv.org/abs/2306.06427

https://arxiv.org/abs/2308.05342

I think the key takeaway is you can't compare LLMs which can be zero/few shot and capable of a huge array of tasks to BERTs which are fine-tuned for a specific task and require annotations, etc. ChatGPT will likely perform as well as gpt2 or bert for the tasks you had mentioned if you apply the concepts like CoT("plan your steps out loud")/prompt engineering(user biases the model)/few-shotting(user prompts the model with "training pairs"). GPT3 and esp 4 will make GPT2 feel hollow if you spend some time turing testing it, but it's also much harder to control LLMs. (I'd say we're engineering a lot of interesting "executive control" tooling as a result and with these more complex pipelines being produced the sky is the limit.) 

It's very likely any one thing a LLM does, a model .1% it's size can be trained to do better. But the fact we can use an LLM on a variety of tasks and it's performance is competitive with a model fine-tuned for that task is a huge win. 

some keywords: distillation, quantization, zero shot, few shot, instruction tuning, fine-tuning, chain of thought, prompt engineering, prompt 


https://arxiv.org/abs/2308.09729
https://arxiv.org/abs/2210.10749
https://arxiv.org/abs/2305.03701

https://arxiv.org/abs/2305.18323

https://arxiv.org/abs/2212.14402

https://arxiv.org/abs/2308.12488
https://arxiv.org/abs/2210.01293
https://arxiv.org/abs/2309.01538
https://arxiv.org/abs/2305.14869